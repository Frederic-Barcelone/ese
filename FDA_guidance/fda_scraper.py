#!/usr/bin/env python3
"""
FDA Guidance Document Scraper - Complete Library (v5)
=====================================================
Single script to build a complete FDA regulatory library.

NEW IN V5:
- FIXED URLS: Updated broken/moved URLs for disease-specific guidances
- IMPROVED ERROR HANDLING: Better 404 handling, skip after first 404 (no retry)
- URL ENCODING: Proper handling of special characters in URLs
- VERIFIED INDEX PAGES: Updated index page URLs
- BETTER LOGGING: More informative status messages
- FALLBACK PDF SEARCH: Direct search on FDA site for missing documents

Features:
- Comprehensive configuration system (CONFIG dict)
- Resume capability: automatically skips already downloaded files
- Global URL tracking: prevents duplicate downloads across categories
- Retry logic with exponential backoff (except for 404s)
- Priority ranking: A (must-have), B (important), C (reference)
- Organized folder structure by category
- Respects FDA robots.txt (configurable delay)
- Complete progress tracking in _progress.json

Usage:
    pip install requests beautifulsoup4
    python fda_scraper_v5.py

Author: Generated by Claude
Version: 5.0
"""

import os
import time
import re
import json
import logging
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs, quote
from typing import Optional, Set, Dict, Tuple

import requests
from bs4 import BeautifulSoup


# =============================================================================
# CONFIGURATION - EDIT THESE AS NEEDED
# =============================================================================

CONFIG = {
    # -------------------------------------------------------------------------
    # Output Settings
    # -------------------------------------------------------------------------
    "output_directory": "FDA_guidance_library",  # Where to save PDFs
    "log_file": "fda_scraper.log",               # Log file name
    
    # -------------------------------------------------------------------------
    # Network Settings
    # -------------------------------------------------------------------------
    "contact_email": "your-email@example.com",   # For User-Agent header
    "request_delay": 30,              # Seconds between requests (FDA requires 30)
    "request_timeout": 60,            # Timeout for page fetches (seconds)
    "download_timeout": 120,          # Timeout for PDF downloads (seconds)
    
    # -------------------------------------------------------------------------
    # Retry Settings
    # -------------------------------------------------------------------------
    "max_retries": 3,                 # Maximum retry attempts per request
    "backoff_base": 5.0,              # Base seconds for exponential backoff
    "backoff_max": 120.0,             # Maximum backoff time (seconds)
    "skip_404_retry": True,           # Don't retry 404 errors (page doesn't exist)
    
    # -------------------------------------------------------------------------
    # Crawl Settings
    # -------------------------------------------------------------------------
    "max_pages_per_index": 10,        # Limit pages per index (None = unlimited)
    "max_documents_total": None,      # Limit total documents (None = unlimited)
    "skip_existing": True,            # Skip files that already exist on disk
    "skip_index_pages": False,        # Skip scanning index pages for additional docs
    "crawl_main_search": False,       # Main search uses JS - set to False by default
    
    # -------------------------------------------------------------------------
    # Logging Settings
    # -------------------------------------------------------------------------
    "verbose": True,                  # Show detailed logs
    "log_to_file": True,              # Write logs to file
    "log_to_console": True,           # Print logs to console
}


# =============================================================================
# LOGGING SETUP
# =============================================================================

def setup_logging() -> logging.Logger:
    """Setup logging based on configuration."""
    log_level = logging.INFO if CONFIG.get("verbose", True) else logging.WARNING
    
    handlers = []
    
    if CONFIG.get("log_to_file", True):
        log_file = CONFIG.get("log_file", "fda_scraper.log")
        handlers.append(logging.FileHandler(log_file))
    
    if CONFIG.get("log_to_console", True):
        handlers.append(logging.StreamHandler())
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=handlers,
        force=True
    )
    
    return logging.getLogger(__name__)


logger = setup_logging()


# =============================================================================
# ALL GUIDANCE DOCUMENTS ORGANIZED BY CATEGORY
# =============================================================================

GUIDANCE_LIBRARY = {
    
    # -------------------------------------------------------------------------
    # 1. RARE DISEASE (CORE)
    # -------------------------------------------------------------------------
    "01_rare_disease": {
        "name": "Rare Disease Drug Development",
        "index_pages": [
            "https://www.fda.gov/drugs/guidances-drugs/guidance-documents-rare-disease-drug-development",
        ],
        "documents": [
            ("A", "Rare Diseases - Considerations for Development of Drugs and Biological Products",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/rare-diseases-considerations-development-drugs-and-biological-products"),
            ("A", "Rare Diseases - Natural History Studies for Drug Development",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/rare-diseases-natural-history-studies-drug-development"),
            ("A", "Rare Diseases - Early Drug Development and Pre-IND Meetings",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/rare-diseases-early-drug-development-and-role-pre-ind-meetings"),
            ("B", "Rare Pediatric Disease Priority Review Vouchers",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/rare-pediatric-disease-priority-review-vouchers"),
            ("B", "Slowly Progressive Low-Prevalence Rare Diseases with Substrate Deposition",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/slowly-progressive-low-prevalence-rare-diseases-substrate-deposition-results-single-enzyme-defects"),
            ("B", "Pediatric Rare Diseases - Gaucher Disease Model",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/pediatric-rare-diseases-collaborative-approach-drug-development-using-gaucher-disease-model-draft"),
            ("B", "Inborn Errors of Metabolism - Dietary Management in Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/inborn-errors-metabolism-use-dietary-management-considerations-optimizing-and-standardizing-diet"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 2. EVIDENCE & BENEFIT-RISK
    # -------------------------------------------------------------------------
    "02_evidence_benefit_risk": {
        "name": "Evidence & Benefit-Risk Framework",
        "index_pages": [],
        "documents": [
            ("A", "Demonstrating Substantial Evidence of Effectiveness",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/demonstrating-substantial-evidence-effectiveness-human-drug-and-biological-products"),
            ("A", "Substantial Evidence With One Adequate Trial and Confirmatory Evidence",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/demonstrating-substantial-evidence-effectiveness-based-one-adequate-and-well-controlled-clinical"),
            ("A", "Benefit-Risk Assessment for New Drug and Biological Products",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/benefit-risk-assessment-new-drug-and-biological-products"),
            ("B", "Providing Clinical Evidence of Effectiveness",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/providing-clinical-evidence-effectiveness-human-drug-and-biological-products"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 3. PATIENT-FOCUSED DRUG DEVELOPMENT (PFDD)
    # -------------------------------------------------------------------------
    "03_patient_focused": {
        "name": "Patient-Focused Drug Development",
        "index_pages": [],
        "documents": [
            ("A", "PFDD Guidance 1 - Collecting Comprehensive and Representative Input",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/patient-focused-drug-development-collecting-comprehensive-and-representative-input"),
            ("A", "PFDD Guidance 2 - Methods to Identify What Is Important to Patients",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/patient-focused-drug-development-methods-identify-what-important-patients"),
            ("A", "PFDD Guidance 3 - Selecting Developing or Modifying Fit-for-Purpose COAs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/patient-focused-drug-development-selecting-developing-or-modifying-fit-purpose-clinical-outcome"),
            ("A", "PFDD Guidance 4 - Incorporating COAs Into Endpoints",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/patient-focused-drug-development-incorporating-clinical-outcome-assessments-endpoints-regulatory"),
            ("B", "Patient-Reported Outcome Measures for Labeling Claims",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/patient-reported-outcome-measures-use-medical-product-development-support-labeling-claims"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 4. REAL-WORLD DATA & EVIDENCE (RWD/RWE)
    # -------------------------------------------------------------------------
    "04_real_world_evidence": {
        "name": "Real-World Data & Evidence",
        "index_pages": [
            "https://www.fda.gov/science-research/science-and-research-special-topics/real-world-evidence",
        ],
        "documents": [
            ("A", "Considerations for Use of RWD and RWE to Support Regulatory Decision-Making",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-use-real-world-data-and-real-world-evidence-support-regulatory-decision-making-drug"),
            ("A", "Submitting Documents Using RWD and RWE to FDA",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/submitting-documents-using-real-world-data-and-real-world-evidence-fda-drug-and-biological-products"),
            ("A", "Real-World Data - Assessing Registries",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/real-world-data-assessing-registries-support-regulatory-decision-making-drug-and-biological-products"),
            ("A", "Considerations for Externally Controlled Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-design-and-conduct-externally-controlled-trials-drug-and-biological-products"),
            ("B", "Real-World Data - Assessing EHR and Medical Claims Data",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/real-world-data-assessing-electronic-health-records-and-medical-claims-data-support-regulatory"),
            ("B", "Use of Electronic Health Record Data in Clinical Investigations",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/use-electronic-health-record-data-clinical-investigations-guidance-industry"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 5. PEDIATRIC DEVELOPMENT
    # -------------------------------------------------------------------------
    "05_pediatric": {
        "name": "Pediatric Drug Development",
        "index_pages": [],
        "documents": [
            ("A", "E11(R1) Clinical Investigation in Pediatric Population",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e11r1-addendum-clinical-investigation-medicinal-products-pediatric-population"),
            ("A", "E11A Pediatric Extrapolation",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e11a-pediatric-extrapolation"),
            ("B", "General Clinical Pharmacology Considerations for Pediatric Studies",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/general-clinical-pharmacology-considerations-pediatric-studies-drugs-including-biological-products"),
            ("B", "General Clinical Pharmacology Considerations for Neonatal Studies",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/general-clinical-pharmacology-considerations-neonatal-studies-drugs-and-biological-products-guidance"),
            ("B", "Ethical Considerations for Clinical Investigations Involving Children",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/ethical-considerations-clinical-investigations-medical-products-involving-children"),
            ("C", "Considerations for Long-Term Neurodevelopmental Safety in Neonates",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-long-term-clinical-neurodevelopmental-safety-studies-neonatal-product-development"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 6. TRIAL DESIGN & STATISTICS
    # -------------------------------------------------------------------------
    "06_trial_design_statistics": {
        "name": "Trial Design & Statistics",
        "index_pages": [],
        "documents": [
            ("A", "Adaptive Design Clinical Trials for Drugs and Biologics",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry"),
            ("A", "Enrichment Strategies for Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/enrichment-strategies-clinical-trials-support-approval-human-drugs-and-biological-products"),
            ("A", "Multiple Endpoints in Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/multiple-endpoints-clinical-trials-guidance-industry"),
            ("A", "Interacting with FDA on Complex Innovative Trial Designs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/interacting-fda-complex-innovative-trial-designs-drugs-and-biological-products"),
            ("B", "Master Protocols for Drug and Biological Product Development",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/master-protocols-drug-and-biological-product-development"),
            ("B", "Non-Inferiority Clinical Trials to Establish Effectiveness",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/non-inferiority-clinical-trials"),
            ("B", "E9 Statistical Principles for Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e9-statistical-principles-clinical-trials"),
            ("B", "E9(R1) Estimands and Sensitivity Analysis",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e9r1-statistical-principles-clinical-trials-addendum-estimands-and-sensitivity-analysis-clinical"),
            ("C", "Adjusting for Covariates in Randomized Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 7. SAFETY & PHARMACOVIGILANCE
    # -------------------------------------------------------------------------
    "07_safety_pharmacovigilance": {
        "name": "Safety & Pharmacovigilance",
        "index_pages": [],
        "documents": [
            ("A", "Safety Reporting Requirements for INDs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/safety-reporting-requirements-inds-investigational-new-drug-applications-and-babe"),
            ("A", "E2F Development Safety Update Report (DSUR)",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e2f-development-safety-update-report"),
            ("B", "Good Pharmacovigilance Practices",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/good-pharmacovigilance-practices-and-pharmacoepidemiologic-assessment"),
            ("B", "Premarketing Risk Assessment",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/premarketing-risk-assessment"),
            ("B", "Development and Use of Risk Minimization Action Plans",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/development-and-use-risk-minimization-action-plans"),
            ("B", "Format and Content of REMS Document",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/format-and-content-rems-document-guidance-industry"),
            ("C", "E2C(R2) Periodic Benefit-Risk Evaluation Report (PBRER)",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e2cr2-periodic-benefit-risk-evaluation-report-pbrer"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 8. DATA STANDARDS & SUBMISSIONS
    # -------------------------------------------------------------------------
    "08_data_standards": {
        "name": "Data Standards & Submissions",
        "index_pages": [],
        "documents": [
            ("A", "Providing Regulatory Submissions - Standardized Study Data",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/providing-regulatory-submissions-electronic-format-standardized-study-data"),
            ("B", "Providing Regulatory Submissions in Electronic Format",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/providing-regulatory-submissions-electronic-format-certain-human-pharmaceutical-product-applications"),
            ("B", "Data Standards for Drug Submissions Containing RWD",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/data-standards-drug-and-biological-product-submissions-containing-real-world-data"),
            ("C", "Electronic Source Data in Clinical Investigations",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/electronic-source-data-clinical-investigations"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 9. EXPEDITED PROGRAMS
    # -------------------------------------------------------------------------
    "09_expedited_programs": {
        "name": "Expedited & Special Programs",
        "index_pages": [],
        "documents": [
            ("A", "Expedited Programs for Serious Conditions - Drugs and Biologics",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/expedited-programs-serious-conditions-drugs-and-biologics"),
            ("A", "Accelerated Approval - Expedited Program for Serious Conditions",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/accelerated-approval-expedited-program-serious-conditions"),
            ("A", "Expedited Programs for Regenerative Medicine Therapies (RMAT)",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/expedited-programs-regenerative-medicine-therapies-serious-conditions"),
            ("B", "Accelerated Approval - Confirmatory Trial Requirements",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/accelerated-approval-and-considerations-determining-whether-confirmatory-trial-underway"),
            ("B", "Expanded Access to Investigational Drugs - Q&A",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/expanded-access-investigational-drugs-treatment-use-questions-and-answers"),
            ("C", "Individual Patient Expanded Access - Form FDA 3926",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/individual-patient-expanded-access-applications-form-fda-3926"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 10. CLINICAL TRIALS & GCP
    # -------------------------------------------------------------------------
    "10_clinical_trials_gcp": {
        "name": "Clinical Trials & GCP",
        "index_pages": [
            "https://www.fda.gov/science-research/clinical-trials-and-human-subject-protection/clinical-trials-guidance-documents",
        ],
        "documents": [
            ("A", "E6(R2) Good Clinical Practice",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e6r2-good-clinical-practice-integrated-addendum-ich-e6r1"),
            ("A", "E8(R1) General Considerations for Clinical Studies",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e8r1-general-considerations-clinical-studies"),
            ("A", "Decentralized Clinical Trials for Drugs Biologics and Devices",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/conducting-clinical-trials-decentralized-elements"),
            ("B", "Digital Health Technologies for Remote Data Acquisition",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/digital-health-technologies-remote-data-acquisition-clinical-investigations"),
            ("B", "E17 Multi-Regional Clinical Trials",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e17-general-principles-planning-and-design-multi-regional-clinical-trials"),
            ("B", "Informed Consent Guidance",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/informed-consent"),
            ("B", "IND Applications by Sponsor-Investigators",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/investigational-new-drug-applications-prepared-and-submitted-sponsor-investigators"),
            ("B", "Best Practices for Communication Between IND Sponsors and FDA",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/best-practices-communication-between-ind-sponsors-and-fda-during-drug-development"),
            ("B", "Formal Meetings Between FDA and Sponsors",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/formal-meetings-between-fda-and-sponsors-or-applicants-pdufa-products"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 11. DISEASE-SPECIFIC (UPDATED URLS - V5)
    # -------------------------------------------------------------------------
    "11_disease_specific": {
        "name": "Disease-Specific Guidances",
        "index_pages": [],
        "documents": [
            ("B", "Amyotrophic Lateral Sclerosis (ALS) - Developing Drugs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/amyotrophic-lateral-sclerosis-developing-drugs-treatment-guidance-industry"),
            ("B", "Duchenne Muscular Dystrophy and Related Dystrophinopathies",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/duchenne-muscular-dystrophy-and-related-dystrophinopathies-developing-drugs-treatment-guidance"),
            # FIXED URL - Added "guidance-industry" suffix
            ("B", "Fabry Disease - Developing Drugs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/fabry-disease-developing-drugs-treatment-guidance-industry"),
            # NOTE: Huntington's and SMA disease-specific guidances may not exist as standalone documents
            # They are covered by general rare disease guidance. Including Patient-Focused Drug Development report instead.
            ("B", "Huntingtons Disease - Patient-Focused Drug Development Report",
             "https://www.fda.gov/media/96350/download"),  # Direct PDF link to PFDD report
        ]
    },
    
    # -------------------------------------------------------------------------
    # 12. BIOMARKERS & DRUG DEVELOPMENT TOOLS
    # -------------------------------------------------------------------------
    "12_biomarkers": {
        "name": "Biomarkers & Drug Development Tools",
        "index_pages": [],
        "documents": [
            ("A", "Biomarker Qualification - Evidentiary Framework",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/biomarker-qualification-evidentiary-framework"),
            ("B", "Qualification Process for Drug Development Tools",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/qualification-process-drug-development-tools-guidance-industry-and-fda-staff"),
            ("B", "Developing Targeted Therapies in Low-Frequency Molecular Subsets",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/developing-targeted-therapies-low-frequency-molecular-subsets-disease"),
            ("C", "Clinical Pharmacogenomics - Premarket Evaluation",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-pharmacogenomics-premarket-evaluation-early-phase-clinical-studies-and-recommendations"),
        ]
    },
    
    # -------------------------------------------------------------------------
    # 13. GENE THERAPY & ADVANCED THERAPIES (UPDATED INDEX URL - V5)
    # -------------------------------------------------------------------------
    "13_gene_therapy": {
        "name": "Gene Therapy & Advanced Therapies",
        "index_pages": [
            # FIXED URL - Updated to correct path
            "https://www.fda.gov/vaccines-blood-biologics/biologics-guidances/cellular-gene-therapy-guidances",
        ],
        "documents": [
            ("A", "Human Gene Therapy for Rare Diseases",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/human-gene-therapy-rare-diseases"),
            ("A", "Innovative Designs for Clinical Trials of CGT Products in Small Populations",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/innovative-designs-clinical-trials-cellular-and-gene-therapy-products-small-populations"),
            ("B", "CMC Information for Human Gene Therapy INDs",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/chemistry-manufacturing-and-control-cmc-information-human-gene-therapy-investigational-new-drug"),
            ("B", "Long Term Follow-Up After Gene Therapy",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/long-term-follow-after-administration-human-gene-therapy-products"),
            ("B", "Early-Phase Clinical Trials of Cellular and Gene Therapy Products",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-design-early-phase-clinical-trials-cellular-and-gene-therapy-products"),
            ("B", "Human Gene Therapy for Neurodegenerative Diseases",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/human-gene-therapy-neurodegenerative-diseases"),
            ("B", "CAR T Cell Products Development",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/considerations-development-chimeric-antigen-receptor-car-t-cell-products"),
            ("C", "Human Gene Therapy Products Incorporating Genome Editing",
             "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/human-gene-therapy-products-incorporating-human-genome-editing"),
        ]
    },
}

# Main FDA guidance search - NOTE: Uses JavaScript table, limited pagination support
MAIN_SEARCH_URL = "https://www.fda.gov/regulatory-information/search-fda-guidance-documents"


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

BASE_URL = "https://www.fda.gov"


def normalize_url(url: str) -> str:
    """Normalize URL for consistent comparison."""
    url = url.lower().strip()
    url = url.rstrip('/')
    url = url.replace('http://', 'https://')
    # Remove query parameters for comparison
    if '?' in url:
        url = url.split('?')[0]
    return url


def slugify(text: str) -> str:
    """Convert text to safe filename."""
    text = re.sub(r'[^\w\s\-]', '', text)
    text = re.sub(r'\s+', '_', text)
    text = re.sub(r'_+', '_', text)
    return text.strip('_')[:150]


def safe_encode_url(url: str) -> str:
    """Safely encode URL with special characters."""
    parsed = urlparse(url)
    # Encode the path component
    encoded_path = quote(parsed.path, safe='/:@')
    # Reconstruct URL
    return f"{parsed.scheme}://{parsed.netloc}{encoded_path}"


def create_session() -> requests.Session:
    """Create HTTP session with proper headers."""
    session = requests.Session()
    contact_email = CONFIG.get("contact_email", "scraper@example.com")
    session.headers.update({
        "User-Agent": f"FDA-Guidance-Scraper/5.0 ({contact_email})",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
    })
    return session


def calculate_backoff(attempt: int) -> float:
    """Calculate exponential backoff time."""
    base = CONFIG.get("backoff_base", 5.0)
    max_backoff = CONFIG.get("backoff_max", 120.0)
    backoff = base * (2 ** attempt)
    return min(backoff, max_backoff)


def fetch_with_retry(
    session: requests.Session,
    url: str,
    timeout_key: str = "request_timeout"
) -> Optional[requests.Response]:
    """
    Fetch a URL with retry logic and exponential backoff.
    
    V5 IMPROVEMENT: Skip retries for 404 errors (page doesn't exist).
    """
    max_retries = CONFIG.get("max_retries", 3)
    timeout = CONFIG.get(timeout_key, 60)
    delay = CONFIG.get("request_delay", 30)
    skip_404_retry = CONFIG.get("skip_404_retry", True)
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                backoff = calculate_backoff(attempt - 1)
                logger.warning(f"Retry {attempt}/{max_retries} after {backoff:.1f}s backoff: {url}")
                time.sleep(backoff)
            
            logger.info(f"Fetching: {url}")
            response = session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # Success - apply standard delay
            time.sleep(delay)
            return response
            
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response is not None else None
            logger.error(f"HTTP {status_code} error for {url}: {e}")
            
            # Don't retry on 404 (page doesn't exist) - V5 improvement
            if status_code == 404:
                if skip_404_retry:
                    logger.warning(f"Page not found (404), skipping retries: {url}")
                    return None
            
            # Don't retry on other 4xx client errors (except 429 rate limit)
            if status_code and 400 <= status_code < 500 and status_code != 429:
                return None
                
        except requests.exceptions.Timeout:
            logger.error(f"Timeout fetching {url} (attempt {attempt + 1}/{max_retries + 1})")
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Connection error for {url}: {e}")
            
        except requests.RequestException as e:
            logger.error(f"Request failed for {url}: {e}")
    
    logger.error(f"All {max_retries + 1} attempts failed for: {url}")
    return None


def get_pdf_url_from_guidance_page(
    session: requests.Session,
    page_url: str
) -> Optional[Dict]:
    """Extract PDF download URL from a guidance detail page."""
    response = fetch_with_retry(session, page_url)
    if not response:
        return None
    
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Get title
    title_elem = soup.find("h1") or soup.find("title")
    title = title_elem.get_text().strip() if title_elem else "Unknown"
    title = title.replace(" | FDA", "").strip()
    
    # Find PDF link - try multiple methods
    pdf_url = None
    
    # Method 1: Links with "download" text
    for link in soup.find_all("a", href=True):
        link_text = (link.get_text() or "").strip().lower()
        href = link["href"]
        
        if any(word in link_text for word in ["download", "pdf", "guidance document"]):
            if ".pdf" in href.lower() or "/media/" in href:
                pdf_url = urljoin(BASE_URL, href)
                break
    
    # Method 2: Any PDF or media link
    if not pdf_url:
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.lower().endswith(".pdf") or ("/media/" in href and "/download" in href):
                pdf_url = urljoin(BASE_URL, href)
                break
    
    if pdf_url:
        return {"title": title, "pdf_url": pdf_url}
    
    logger.warning(f"No PDF found on: {page_url}")
    return None


def download_pdf_with_retry(
    session: requests.Session,
    pdf_url: str,
    save_path: str
) -> bool:
    """Download a PDF file with retry logic."""
    # Skip if already exists (RESUME CAPABILITY)
    if CONFIG.get("skip_existing", True) and os.path.exists(save_path):
        logger.info(f"Already exists, skipping: {os.path.basename(save_path)}")
        return True
    
    max_retries = CONFIG.get("max_retries", 3)
    timeout = CONFIG.get("download_timeout", 120)
    delay = CONFIG.get("request_delay", 30)
    
    # Safely encode URL
    encoded_url = safe_encode_url(pdf_url)
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                backoff = calculate_backoff(attempt - 1)
                logger.warning(f"Retry {attempt}/{max_retries} after {backoff:.1f}s backoff for download")
                time.sleep(backoff)
            
            logger.info(f"Downloading: {os.path.basename(save_path)}")
            
            with session.get(encoded_url, stream=True, timeout=timeout) as response:
                response.raise_for_status()
                
                # Write to temp file first
                temp_path = save_path + ".tmp"
                with open(temp_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                # Rename to final path on success
                os.rename(temp_path, save_path)
            
            time.sleep(delay)
            return True
            
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response is not None else None
            logger.error(f"Download attempt {attempt + 1} failed (HTTP {status_code}): {e}")
            
            # Don't retry 404s
            if status_code == 404:
                logger.warning(f"PDF not found (404), skipping: {pdf_url}")
                break
                
        except Exception as e:
            logger.error(f"Download attempt {attempt + 1} failed: {e}")
            
        # Clean up partial/temp files
        for path in [save_path, save_path + ".tmp"]:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except OSError:
                    pass
    
    logger.error(f"All {max_retries + 1} download attempts failed for: {pdf_url}")
    return False


def extract_guidance_links(soup: BeautifulSoup) -> Set[str]:
    """Extract guidance document links from a page."""
    links = set()
    main = soup.find("main") or soup.find("div", {"role": "main"}) or soup
    
    for link in main.find_all("a", href=True):
        href = link["href"]
        
        if "/regulatory-information/search-fda-guidance-documents/" in href:
            full_url = urljoin(BASE_URL, href)
            # Exclude pagination links
            if "?page=" not in full_url:
                links.add(full_url)
        elif href.lower().endswith(".pdf"):
            links.add(urljoin(BASE_URL, href))
        elif "/media/" in href and "/download" in href:
            links.add(urljoin(BASE_URL, href))
    
    return links


def find_next_page_url(soup: BeautifulSoup, current_url: str) -> Optional[str]:
    """
    Find the "Next" page link for pagination.
    
    FDA uses various pagination patterns:
    - ?page=N query parameter
    - "Next" or "›" links
    - Pagination nav elements
    """
    # Look for pagination container
    pagination = soup.find("nav", {"class": re.compile(r"pager|pagination", re.I)})
    if not pagination:
        pagination = soup.find("ul", {"class": re.compile(r"pager|pagination", re.I)})
    if not pagination:
        pagination = soup.find("div", {"class": re.compile(r"pager|pagination", re.I)})
    
    search_area = pagination if pagination else soup
    
    # Look for "Next" links
    for link in search_area.find_all("a", href=True):
        link_text = link.get_text().strip().lower()
        link_class = " ".join(link.get("class", []))
        link_rel = link.get("rel", [])
        
        # Check for next indicators
        is_next = (
            "next" in link_text or
            "›" in link_text or
            "»" in link_text or
            "next" in link_class.lower() or
            "next" in link_rel
        )
        
        if is_next:
            href = link["href"]
            # Skip disabled/current page links
            if href and href != "#" and "javascript:" not in href.lower():
                return urljoin(BASE_URL, href)
    
    # Fallback: Look for page=N+1 pattern
    parsed = urlparse(current_url)
    query_params = parse_qs(parsed.query)
    current_page = int(query_params.get("page", ["0"])[0])
    
    # Look for link to next page number
    for link in search_area.find_all("a", href=True):
        href = link["href"]
        if f"page={current_page + 1}" in href:
            return urljoin(BASE_URL, href)
    
    return None


def crawl_index_with_pagination(
    session: requests.Session,
    start_url: str,
    max_pages: Optional[int] = None
) -> Tuple[Set[str], int]:
    """
    Crawl an index page and follow pagination to get ALL guidance links.
    
    Args:
        session: requests Session
        start_url: Starting URL of the index
        max_pages: Maximum pages to crawl (None = unlimited)
    
    Returns:
        Tuple of (set of guidance URLs, number of pages crawled)
    """
    all_links = set()
    current_url = start_url
    pages_crawled = 0
    visited_pages = set()
    
    while current_url:
        # Avoid infinite loops
        normalized_page = normalize_url(current_url)
        if normalized_page in visited_pages:
            logger.warning(f"Already visited page, stopping: {current_url}")
            break
        visited_pages.add(normalized_page)
        
        # Check page limit
        if max_pages and pages_crawled >= max_pages:
            logger.info(f"Reached max_pages_per_index limit ({max_pages})")
            break
        
        # Fetch the page
        response = fetch_with_retry(session, current_url)
        if not response:
            break
        
        pages_crawled += 1
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Extract guidance links
        page_links = extract_guidance_links(soup)
        new_links = page_links - all_links
        all_links.update(page_links)
        
        logger.info(f"Page {pages_crawled}: Found {len(page_links)} links ({len(new_links)} new)")
        
        # Find next page
        next_url = find_next_page_url(soup, current_url)
        
        if next_url:
            logger.info(f"Found next page: {next_url}")
            current_url = next_url
        else:
            logger.info("No more pages found")
            current_url = None
    
    return all_links, pages_crawled


# =============================================================================
# PROGRESS TRACKING
# =============================================================================

def load_progress(progress_file: str) -> Dict:
    """Load progress from file."""
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r') as f:
                data = json.load(f)
                for key in ["downloaded", "failed", "processed_urls", "last_category"]:
                    if key not in data:
                        data[key] = [] if key != "last_category" else None
                return data
        except (OSError, json.JSONDecodeError):
            pass
    return {
        "downloaded": [],
        "failed": [],
        "processed_urls": [],
        "last_category": None
    }


def save_progress(progress_file: str, progress: Dict):
    """Save progress to file."""
    with open(progress_file, 'w') as f:
        json.dump(progress, f, indent=2)


def build_global_url_set(progress: Dict) -> Set[str]:
    """Build set of all URLs that have been processed."""
    urls = set()
    
    for url in progress.get("processed_urls", []):
        urls.add(normalize_url(url))
    
    for item in progress.get("downloaded", []):
        if "url" in item:
            urls.add(normalize_url(item["url"]))
    
    return urls


def collect_all_defined_urls() -> Set[str]:
    """Collect all URLs defined in GUIDANCE_LIBRARY."""
    urls = set()
    for category in GUIDANCE_LIBRARY.values():
        for priority, title, url in category.get("documents", []):
            urls.add(normalize_url(url))
    return urls


# =============================================================================
# MAIN FUNCTION
# =============================================================================

def main():
    """Main scraper function."""
    
    start_time = datetime.now()
    output_folder = CONFIG.get("output_directory", "FDA_guidance_library")
    
    print("=" * 70)
    print("FDA GUIDANCE DOCUMENT SCRAPER (v5 - IMPROVED)")
    print("=" * 70)
    print(f"Started: {start_time}")
    print(f"Output folder: {output_folder}")
    print()
    print("Configuration:")
    print(f"  Request delay: {CONFIG.get('request_delay', 30)}s")
    print(f"  Max pages per index: {CONFIG.get('max_pages_per_index') or 'UNLIMITED'}")
    print(f"  Max retries: {CONFIG.get('max_retries', 3)}")
    print(f"  Skip existing: {CONFIG.get('skip_existing', True)}")
    print(f"  Skip 404 retry: {CONFIG.get('skip_404_retry', True)}")
    print(f"  Skip index pages: {CONFIG.get('skip_index_pages', False)}")
    print(f"  Crawl main search: {CONFIG.get('crawl_main_search', False)}")
    print(f"  Max documents: {CONFIG.get('max_documents_total') or 'UNLIMITED'}")
    print(f"  Categories: {len(GUIDANCE_LIBRARY)}")
    print("=" * 70)
    print()
    
    # Create output folder
    os.makedirs(output_folder, exist_ok=True)
    
    # Progress tracking
    progress_file = os.path.join(output_folder, "_progress.json")
    progress = load_progress(progress_file)
    
    global_processed_urls = build_global_url_set(progress)
    print(f"Previously processed URLs: {len(global_processed_urls)}")
    
    all_defined_urls = collect_all_defined_urls()
    print(f"Explicitly defined documents: {len(all_defined_urls)}")
    print()
    
    # Create session
    session = create_session()
    
    # Statistics
    stats = {
        "total_attempted": 0,
        "downloaded": 0,
        "skipped_existing": 0,
        "skipped_duplicate": 0,
        "failed": 0,
        "by_priority": {"A": 0, "B": 0, "C": 0},
        "pages_crawled": 0
    }
    
    max_docs = CONFIG.get("max_documents_total")
    max_pages = CONFIG.get("max_pages_per_index")
    skip_index = CONFIG.get("skip_index_pages", False)
    
    # ==========================================================================
    # PHASE 1: Process explicitly defined documents in each category
    # ==========================================================================
    print("=" * 70)
    print("PHASE 1: Processing explicitly defined documents")
    print("=" * 70)
    
    for folder_name, category in GUIDANCE_LIBRARY.items():
        if max_docs and stats["downloaded"] >= max_docs:
            print(f"\nReached max_documents_total limit ({max_docs})")
            break
        
        print()
        print("-" * 70)
        print(f"CATEGORY: {category['name']}")
        print(f"Folder: {folder_name}")
        print("-" * 70)
        
        category_path = os.path.join(output_folder, folder_name)
        os.makedirs(category_path, exist_ok=True)
        
        print(f"\nProcessing {len(category['documents'])} specific documents...")
        
        for priority, title, url in category['documents']:
            if max_docs and stats["downloaded"] >= max_docs:
                break
                
            stats["total_attempted"] += 1
            normalized_url = normalize_url(url)
            
            filename = f"{priority}_{slugify(title)}.pdf"
            save_path = os.path.join(category_path, filename)
            
            # Check if already downloaded
            if CONFIG.get("skip_existing", True) and os.path.exists(save_path):
                print(f"  [SKIP] {filename[:60]}...")
                stats["skipped_existing"] += 1
                stats["by_priority"][priority] += 1
                if normalized_url not in global_processed_urls:
                    global_processed_urls.add(normalized_url)
                    progress["processed_urls"].append(url)
                continue
            
            if normalized_url in global_processed_urls:
                print(f"  [DUP] {title[:55]}...")
                stats["skipped_duplicate"] += 1
                continue
            
            print(f"  [{priority}] {title[:55]}...")
            
            # Get PDF URL - check if it's already a direct PDF/media link
            if url.lower().endswith(".pdf") or "/media/" in url:
                pdf_url = url
            else:
                pdf_info = get_pdf_url_from_guidance_page(session, url)
                if not pdf_info:
                    stats["failed"] += 1
                    progress["failed"].append({
                        "title": title, "url": url, "category": folder_name,
                        "reason": "Could not find PDF on page"
                    })
                    global_processed_urls.add(normalized_url)
                    progress["processed_urls"].append(url)
                    save_progress(progress_file, progress)
                    continue
                pdf_url = pdf_info["pdf_url"]

            # Download
            if download_pdf_with_retry(session, pdf_url, save_path):
                stats["downloaded"] += 1
                stats["by_priority"][priority] += 1
                progress["downloaded"].append({
                    "title": title,
                    "url": url,
                    "path": save_path,
                    "priority": priority,
                    "category": folder_name
                })
                global_processed_urls.add(normalized_url)
                progress["processed_urls"].append(url)
            else:
                stats["failed"] += 1
                progress["failed"].append({
                    "title": title, "url": url, "category": folder_name,
                    "reason": "Download failed"
                })
                global_processed_urls.add(normalized_url)
                progress["processed_urls"].append(url)
            
            save_progress(progress_file, progress)
        
        progress["last_category"] = folder_name
        save_progress(progress_file, progress)
    
    # ==========================================================================
    # PHASE 2: Deep crawl index pages with pagination
    # ==========================================================================
    if not skip_index:
        print()
        print("=" * 70)
        print("PHASE 2: Deep crawling index pages with pagination")
        print("=" * 70)
        
        for folder_name, category in GUIDANCE_LIBRARY.items():
            if max_docs and stats["downloaded"] >= max_docs:
                break
            
            if not category.get("index_pages"):
                continue
            
            category_path = os.path.join(output_folder, folder_name)
            os.makedirs(category_path, exist_ok=True)
            
            for index_url in category["index_pages"]:
                if max_docs and stats["downloaded"] >= max_docs:
                    break
                
                print()
                print(f"\n📄 Deep crawling: {index_url[:70]}...")
                print(f"   Category: {category['name']}")
                
                # Crawl with pagination
                all_links, pages = crawl_index_with_pagination(
                    session, index_url, max_pages
                )
                stats["pages_crawled"] += pages
                
                # Filter to new links only
                new_links = []
                for link in all_links:
                    normalized = normalize_url(link)
                    if normalized not in global_processed_urls and normalized not in all_defined_urls:
                        new_links.append(link)
                
                print(f"   Pages crawled: {pages}")
                print(f"   Total links found: {len(all_links)}")
                print(f"   New links to process: {len(new_links)}")
                
                # Process new links
                for link in sorted(new_links):
                    if max_docs and stats["downloaded"] >= max_docs:
                        break
                    
                    stats["total_attempted"] += 1
                    normalized_link = normalize_url(link)
                    
                    if normalized_link in global_processed_urls:
                        stats["skipped_duplicate"] += 1
                        continue
                    
                    priority = "C"
                    
                    if link.lower().endswith(".pdf") or "/media/" in link:
                        title = os.path.basename(urlparse(link).path).replace(".pdf", "")
                        filename = f"{priority}_{slugify(title)}.pdf"
                        save_path = os.path.join(category_path, filename)
                        
                        if CONFIG.get("skip_existing", True) and os.path.exists(save_path):
                            stats["skipped_existing"] += 1
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                            save_progress(progress_file, progress)
                            continue
                        
                        if download_pdf_with_retry(session, link, save_path):
                            stats["downloaded"] += 1
                            stats["by_priority"][priority] += 1
                            progress["downloaded"].append({
                                "title": title,
                                "url": link,
                                "path": save_path,
                                "priority": priority,
                                "category": folder_name,
                                "source": "index_page"
                            })
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                        else:
                            stats["failed"] += 1
                            progress["failed"].append({"url": link, "category": folder_name})
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                    else:
                        pdf_info = get_pdf_url_from_guidance_page(session, link)
                        if not pdf_info:
                            stats["failed"] += 1
                            progress["failed"].append({"url": link, "category": folder_name})
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                            save_progress(progress_file, progress)
                            continue
                        
                        filename = f"{priority}_{slugify(pdf_info['title'])}.pdf"
                        save_path = os.path.join(category_path, filename)
                        
                        if CONFIG.get("skip_existing", True) and os.path.exists(save_path):
                            stats["skipped_existing"] += 1
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                            save_progress(progress_file, progress)
                            continue
                        
                        if download_pdf_with_retry(session, pdf_info["pdf_url"], save_path):
                            stats["downloaded"] += 1
                            stats["by_priority"][priority] += 1
                            progress["downloaded"].append({
                                "title": pdf_info["title"],
                                "url": link,
                                "path": save_path,
                                "priority": priority,
                                "category": folder_name,
                                "source": "index_page"
                            })
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                        else:
                            stats["failed"] += 1
                            progress["failed"].append({"url": link, "category": folder_name})
                            global_processed_urls.add(normalized_link)
                            progress["processed_urls"].append(link)
                    
                    save_progress(progress_file, progress)
    
    # Final summary
    end_time = datetime.now()
    duration = end_time - start_time
    
    print()
    print("=" * 70)
    print("SCRAPING COMPLETE")
    print("=" * 70)
    print(f"Duration: {duration}")
    print(f"Total pages crawled: {stats['pages_crawled']}")
    print(f"Total attempted: {stats['total_attempted']}")
    print(f"Downloaded: {stats['downloaded']}")
    print(f"Skipped (existing): {stats['skipped_existing']}")
    print(f"Skipped (duplicates): {stats['skipped_duplicate']}")
    print(f"Failed: {stats['failed']}")
    print()
    print("By Priority:")
    print(f"  A (Must-have): {stats['by_priority']['A']}")
    print(f"  B (Important): {stats['by_priority']['B']}")
    print(f"  C (Reference): {stats['by_priority']['C']}")
    print()
    print(f"Total unique URLs processed: {len(global_processed_urls)}")
    print(f"Output folder: {os.path.abspath(output_folder)}")
    print("=" * 70)
    
    # Print failed URLs for easy review
    if progress.get("failed"):
        print()
        print("FAILED URLS (may need manual review):")
        print("-" * 70)
        for item in progress["failed"][-20:]:  # Show last 20 failures
            if isinstance(item, dict):
                url = item.get("url", "Unknown")
                reason = item.get("reason", "Unknown")
                print(f"  - {url[:60]}...")
                print(f"    Reason: {reason}")
    
    # Save manifest
    manifest = {
        "completed_at": end_time.isoformat(),
        "duration_seconds": duration.total_seconds(),
        "statistics": stats,
        "configuration": {k: v for k, v in CONFIG.items()},
        "categories": list(GUIDANCE_LIBRARY.keys()),
        "total_unique_urls": len(global_processed_urls),
        "version": "5.0"
    }
    
    manifest_path = os.path.join(output_folder, "manifest.json")
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)
    
    print(f"\nManifest saved to: {manifest_path}")
    print("To resume if interrupted, just run the script again!")


if __name__ == "__main__":
    main()