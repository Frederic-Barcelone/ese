#!/usr/bin/env python3
# corpus_metadata/corpus_metadata/F_evaluation/F03_generator_unit_test.py
"""
================================================================================
NLP4RARE PIPELINE EVALUATION TEST
================================================================================

PURPOSE:
    End-to-end evaluation of the abbreviation extraction pipeline against
    the NLP4RARE gold standard corpus. This script validates that the full
    pipeline (parsing → generation → validation → normalization) correctly
    extracts abbreviation-definition pairs from medical documents.

WHAT IT DOES:
    1. LOAD GOLD STANDARD
       - Reads nlp4rare_gold.json containing human-annotated abbreviation pairs
       - Groups annotations by document (e.g., "Acanthosis-Nigricans.pdf")
       - Each gold entry has: short_form, long_form, category, doc_id

    2. PROCESS PDFs
       - For each PDF file in the NLP4RARE corpus (dev/test/train folders)
       - Runs the full Orchestrator pipeline:
         * B_parsing: PDF → DocumentGraph (layout-aware text extraction)
         * C_generators: Candidate generation (syntax, regex, lexicon, glossary)
         * D_validation: LLM verification (optional, can be skipped for speed)
         * E_normalization: Term mapping and disambiguation
       - Collects validated abbreviation extractions

    3. COMPARE RESULTS
       - For each document, compares extracted abbreviations vs gold standard
       - Matching logic:
         * Short Form: Exact match (case-insensitive)
         * Long Form: Fuzzy match (exact, substring, or 80% similarity)
       - Classifies each result as TP (correct), FP (extra), or FN (missed)

    4. CALCULATE METRICS
       - Per-document: TP, FP, FN, Precision, Recall, F1
       - Per-split (dev/test/train): Aggregate metrics
       - Overall corpus: Combined metrics across all splits

METRICS EXPLAINED:
    TP (True Positive):  System extracted abbreviation that matches gold standard
    FP (False Positive): System extracted abbreviation NOT in gold (noise/hallucination)
    FN (False Negative): Gold standard abbreviation NOT extracted (missed)
    
    Precision = TP / (TP + FP)  → "Of what we found, how much is correct?"
    Recall    = TP / (TP + FN)  → "Of what exists, how much did we find?"
    F1 Score  = 2 * P * R / (P + R)  → Harmonic mean (balance of P and R)

OUTPUT:
    Console only - prints detailed progress and final metrics report including:
    - Per-document extraction counts and metrics
    - Per-split summary tables
    - Overall corpus performance
    - Error analysis with sample FN (missed) and FP (extra) pairs

CONFIGURATION:
    All parameters are hardcoded in the CONFIGURATION section below.
    Modify MAX_DOCS to control how many documents to process (for testing).
    Set SKIP_VALIDATION=True for faster runs (skips LLM verification).

USAGE:
    python F04_pipeline_test.py

DEPENDENCIES:
    - Orchestrator and all pipeline components (A_core through E_normalization)
    - nlp4rare_gold.json (generated by generate_nlp4rare_gold.py)
    - NLP4RARE PDF files in dev/test/train folders

================================================================================
"""

import json
import sys
import time
from dataclasses import dataclass, field
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, TYPE_CHECKING

# Add corpus_metadata to path once at module load
_BASE_PATH = Path("/Users/frederictetard/Projects/ese")
if str(_BASE_PATH / "corpus_metadata") not in sys.path:
    sys.path.insert(0, str(_BASE_PATH / "corpus_metadata"))

from orchestrator import Orchestrator
from A_core.A01_domain_models import ValidationStatus

if TYPE_CHECKING:
    pass  # For any type-only imports

# =============================================================================
# CONFIGURATION (ALL HARDCODED - MODIFY HERE)
# =============================================================================

BASE_PATH = Path("/Users/frederictetard/Projects/ese")
GOLD_JSON = BASE_PATH / "gold_data" / "nlp4rare_gold.json"
NLP4RARE_PATH = BASE_PATH / "gold_data" / "NLP4RARE"
CONFIG_PATH = BASE_PATH / "corpus_metadata" / "G_config" / "config.yaml"

# Evaluation settings
MAX_DOCS = 5                    # Max documents to process per split (None = all)
SKIP_VALIDATION = False         # Enable LLM validation for proper evaluation
SPLITS_TO_EVALUATE = ["dev"]    # Which splits to run: ["dev"], ["test"], ["train"], or all
FUZZY_THRESHOLD = 0.8           # Long form matching threshold (0.8 = 80% similarity)
GOLD_CATEGORIES = ["DISEASE", "RAREDISEASE", "SKINRAREDISEASE"]  # Categories to evaluate (None = all)
EVAL_ABBREV_CLASSES = ["disease"]  # Only evaluate these abbreviation classes (disease, gene, identifier, general_medical, other)


# =============================================================================
# ABBREVIATION CLASSIFICATION
# =============================================================================

from enum import Enum
import re


class AbbreviationClass(Enum):
    """Classification of extracted abbreviations."""
    DISEASE = "disease"              # Disease abbreviations (target for evaluation)
    GENE = "gene"                    # Gene symbols (JAG1, NOTCH2, BRCA1, etc.)
    IDENTIFIER = "identifier"        # Database IDs (OMIM, PMID, DOI, NCT, etc.)
    GENERAL_MEDICAL = "general_medical"  # General medical terms (MRI, CT, ECG, etc.)
    OTHER = "other"                  # Unclassified


# Gene symbol patterns (typically 2-6 uppercase letters followed by optional numbers)
GENE_PATTERNS = [
    r'^[A-Z]{2,6}\d{0,2}$',          # JAG1, NOTCH2, BRCA1, TP53, etc.
    r'^[A-Z]{2,4}-[A-Z]{1,3}\d?$',   # HLA-B, HLA-DR, etc.
]

# Known gene symbols (common ones that might be ambiguous)
KNOWN_GENES = {
    "JAG1", "JAG2", "NOTCH1", "NOTCH2", "NOTCH3", "NOTCH4",
    "BRCA1", "BRCA2", "TP53", "EGFR", "KRAS", "BRAF", "ALK",
    "HER2", "ERBB2", "MYC", "RAS", "RAF", "MEK", "ERK",
    "CFTR", "DMD", "SMN1", "SMN2", "HTT", "FMR1", "PKD1", "PKD2",
    "COL1A1", "COL1A2", "FBN1", "FGFR1", "FGFR2", "FGFR3",
    "NF1", "NF2", "TSC1", "TSC2", "VHL", "RB1", "APC", "MLH1",
}

# Identifier patterns
IDENTIFIER_PATTERNS = {
    "OMIM": r'^OMIM$',
    "ORPHA": r'^ORPHA(NET)?$',
    "PMID": r'^PMID$',
    "DOI": r'^DOI$',
    "NCT": r'^NCT\d*$',
    "ISRCTN": r'^ISRCTN$',
    "EUDRACT": r'^EUDRACT$',
    "ICD": r'^ICD-?\d{0,2}$',
    "SNOMED": r'^SNOMED$',
    "MESH": r'^MESH$',
    "MONDO": r'^MONDO$',
    "UMLS": r'^UMLS$',
    "CUI": r'^C\d{7}$',               # UMLS CUI format
}

# General medical abbreviations (non-disease)
GENERAL_MEDICAL = {
    # Imaging
    "MRI", "CT", "PET", "SPECT", "MRA", "CTA", "DSA", "EEG", "EMG", "ECG", "EKG",
    "US", "USG", "ECHO", "TEE", "TTE",
    # Lab/procedures
    "CBC", "BMP", "CMP", "LFT", "RFT", "ABG", "VBG", "CSF", "BAL",
    "ELISA", "PCR", "WB", "IHC", "FISH", "NGS", "WES", "WGS",
    # Clinical
    "IV", "IM", "SC", "PO", "PR", "SL", "INH", "TOP",
    "BID", "TID", "QID", "PRN", "QD", "QOD", "QHS",
    "BMI", "BP", "HR", "RR", "SPO2", "GCS", "APACHE",
    # Statistical
    "CI", "SD", "SE", "SEM", "OR", "RR", "HR", "IQR", "AUC", "ROC",
    "NS", "NA", "NR", "ND",
    # Organizations/standards
    "FDA", "EMA", "WHO", "CDC", "NIH", "NHS",
    "IRB", "DSMB", "GCP", "ICH",
}

# Disease-related keywords in long forms
DISEASE_KEYWORDS = [
    "disease", "syndrome", "disorder", "deficiency", "dystrophy",
    "neuropathy", "myopathy", "cardiomyopathy", "encephalopathy",
    "anemia", "leukemia", "lymphoma", "sarcoma", "carcinoma",
    "sclerosis", "fibrosis", "cirrhosis", "necrosis",
    "itis",  # suffix for inflammation
    "osis",  # suffix for condition
    "pathy", # suffix for disease
]


def classify_abbreviation(short_form: str, long_form: Optional[str], lexicon_source: Optional[str] = None) -> AbbreviationClass:
    """Classify an abbreviation into Disease, Gene, Identifier, General Medical, or Other.

    Args:
        short_form: The abbreviation
        long_form: The expansion (if available)
        lexicon_source: Source lexicon file name (if available)

    Returns:
        AbbreviationClass enum value
    """
    sf_upper = short_form.strip().upper()

    # 1. Check identifiers first (highest priority)
    for id_type, pattern in IDENTIFIER_PATTERNS.items():
        if re.match(pattern, sf_upper, re.IGNORECASE):
            return AbbreviationClass.IDENTIFIER

    # 2. Check known genes
    if sf_upper in KNOWN_GENES:
        return AbbreviationClass.GENE

    # 3. Check gene patterns (if no long form - genes typically don't have expansions in text)
    if not long_form or long_form.strip() == "":
        for pattern in GENE_PATTERNS:
            if re.match(pattern, sf_upper):
                # Additional check: genes are typically 2-6 chars, mostly consonants
                if len(sf_upper) <= 6 and sum(1 for c in sf_upper if c in 'AEIOU') <= 2:
                    return AbbreviationClass.GENE

    # 4. Check general medical abbreviations
    if sf_upper in GENERAL_MEDICAL:
        return AbbreviationClass.GENERAL_MEDICAL

    # 5. Check if long form contains disease keywords
    if long_form:
        lf_lower = long_form.lower()
        for keyword in DISEASE_KEYWORDS:
            if keyword in lf_lower:
                return AbbreviationClass.DISEASE

    # 6. Check lexicon source for disease lexicons
    if lexicon_source:
        ls_lower = lexicon_source.lower()
        if any(term in ls_lower for term in ["disease", "orphan", "rare"]):
            return AbbreviationClass.DISEASE

    # 7. If has a long form but didn't match anything, likely disease or other medical
    if long_form and long_form.strip():
        # Default to disease for abbreviations with expansions that weren't classified otherwise
        return AbbreviationClass.DISEASE

    return AbbreviationClass.OTHER


# =============================================================================
# DATA STRUCTURES
# =============================================================================


@dataclass
class GoldEntry:
    """Single gold standard annotation."""
    short_form: str
    long_form: str
    category: str
    doc_id: str

    @property
    def sf_normalized(self) -> str:
        return self.short_form.strip().upper()

    @property
    def lf_normalized(self) -> str:
        return " ".join(self.long_form.strip().lower().split())


@dataclass
class ExtractedEntry:
    """Single extracted abbreviation with classification."""
    short_form: str
    long_form: Optional[str]
    confidence: float = 0.0
    abbrev_class: AbbreviationClass = AbbreviationClass.OTHER
    lexicon_source: Optional[str] = None

    @property
    def sf_normalized(self) -> str:
        return self.short_form.strip().upper()

    @property
    def lf_normalized(self) -> Optional[str]:
        if not self.long_form:
            return None
        return " ".join(self.long_form.strip().lower().split())

    @property
    def is_disease(self) -> bool:
        return self.abbrev_class == AbbreviationClass.DISEASE


@dataclass
class DocumentMetrics:
    """Metrics for a single document."""
    doc_id: str
    tp: int = 0
    fp: int = 0
    fn: int = 0
    gold_count: int = 0
    extracted_count: int = 0
    processing_time: float = 0.0
    error: Optional[str] = None
    tp_pairs: List[Tuple[str, str]] = field(default_factory=list)
    fp_pairs: List[Tuple[str, str]] = field(default_factory=list)
    fn_pairs: List[Tuple[str, str]] = field(default_factory=list)

    @property
    def precision(self) -> float:
        return self.tp / (self.tp + self.fp) if (self.tp + self.fp) > 0 else 0.0

    @property
    def recall(self) -> float:
        return self.tp / (self.tp + self.fn) if (self.tp + self.fn) > 0 else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


@dataclass
class AggregateMetrics:
    """Aggregate metrics across multiple documents."""
    split_name: str
    total_docs: int = 0
    processed_docs: int = 0
    failed_docs: int = 0
    total_tp: int = 0
    total_fp: int = 0
    total_fn: int = 0
    total_gold: int = 0
    total_extracted: int = 0
    total_time: float = 0.0

    @property
    def precision(self) -> float:
        return self.total_tp / (self.total_tp + self.total_fp) if (self.total_tp + self.total_fp) > 0 else 0.0

    @property
    def recall(self) -> float:
        return self.total_tp / (self.total_tp + self.total_fn) if (self.total_tp + self.total_fn) > 0 else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


# =============================================================================
# GOLD STANDARD LOADER
# =============================================================================


def load_gold_standard(gold_path: Path, categories: Optional[List[str]] = None) -> Dict[str, List[GoldEntry]]:
    """Load gold standard and group by document ID.

    Args:
        gold_path: Path to gold standard JSON file
        categories: If provided, only include entries with these categories
    """
    if not gold_path.exists():
        raise FileNotFoundError(f"Gold standard not found: {gold_path}")

    with open(gold_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    annotations = data.get("annotations", [])

    by_doc: Dict[str, List[GoldEntry]] = {}
    filtered_count = 0
    for ann in annotations:
        category = ann.get("category", "UNKNOWN")

        # Filter by category if specified
        if categories and category not in categories:
            filtered_count += 1
            continue

        entry = GoldEntry(
            short_form=ann["short_form"],
            long_form=ann["long_form"],
            category=category,
            doc_id=ann["doc_id"],
        )
        by_doc.setdefault(entry.doc_id, []).append(entry)

    if filtered_count > 0:
        print(f" Filtered out {filtered_count} entries (categories not in {categories})")

    return by_doc


# =============================================================================
# MATCHING LOGIC
# =============================================================================


def normalize_medical_synonyms(text: str) -> str:
    """Normalize common medical synonyms for better matching."""
    # Common interchangeable terms in medical contexts
    synonyms = [
        ("disease", "syndrome"),
        ("disorder", "syndrome"),
        ("disease (disorder)", "syndrome"),
        ("deficiency", "deficit"),
    ]
    normalized = text.lower()
    for term1, term2 in synonyms:
        # Normalize to first term for comparison
        normalized = normalized.replace(term2, term1)
    return normalized


def lf_matches(sys_lf: Optional[str], gold_lf: Optional[str], threshold: float = 0.8) -> bool:
    """Check if long forms match (exact, substring, fuzzy, or synonym-normalized)."""
    if sys_lf is None and gold_lf is None:
        return True
    if sys_lf is None or gold_lf is None:
        return False

    sys_norm = " ".join(sys_lf.strip().lower().split())
    gold_norm = " ".join(gold_lf.strip().lower().split())

    # Exact match
    if sys_norm == gold_norm:
        return True

    # Substring match
    if sys_norm in gold_norm or gold_norm in sys_norm:
        return True

    # Synonym-normalized match (disease ↔ syndrome, etc.)
    sys_syn = normalize_medical_synonyms(sys_norm)
    gold_syn = normalize_medical_synonyms(gold_norm)
    if sys_syn == gold_syn:
        return True
    if sys_syn in gold_syn or gold_syn in sys_syn:
        return True

    # Fuzzy match on normalized forms
    ratio = SequenceMatcher(None, sys_syn, gold_syn).ratio()
    return ratio >= threshold


def compare_extractions(
    extracted: List[ExtractedEntry],
    gold: List[GoldEntry],
    eval_classes: Optional[List[str]] = None,
) -> Tuple[DocumentMetrics, Dict[str, int]]:
    """Compare extracted abbreviations against gold standard.

    Args:
        extracted: List of extracted abbreviations
        gold: List of gold standard entries
        eval_classes: List of abbreviation classes to evaluate (e.g., ["disease"])
                     If None, evaluate all classes

    Returns:
        Tuple of (DocumentMetrics, class_counts dict)
    """
    doc_id = gold[0].doc_id if gold else "unknown"
    metrics = DocumentMetrics(doc_id=doc_id)
    metrics.gold_count = len(gold)

    # Count extractions by class
    class_counts: Dict[str, int] = {}
    for ext in extracted:
        class_name = ext.abbrev_class.value
        class_counts[class_name] = class_counts.get(class_name, 0) + 1

    # Filter extracted to only include specified classes
    if eval_classes:
        filtered_extracted = [
            ext for ext in extracted
            if ext.abbrev_class.value in eval_classes
        ]
        filtered_count = len(extracted) - len(filtered_extracted)
        if filtered_count > 0:
            print(f"      Classification filter: {len(extracted)} total -> {len(filtered_extracted)} disease-class (filtered {filtered_count})")
            for class_name, count in sorted(class_counts.items()):
                marker = "✓" if class_name in eval_classes else "✗"
                print(f"        {marker} {class_name}: {count}")
    else:
        filtered_extracted = extracted

    metrics.extracted_count = len(filtered_extracted)

    gold_by_sf: Dict[str, List[GoldEntry]] = {}
    for g in gold:
        gold_by_sf.setdefault(g.sf_normalized, []).append(g)

    matched_gold: Set[Tuple[str, str]] = set()

    for ext in filtered_extracted:
        sf_norm = ext.sf_normalized
        matched = False

        if sf_norm in gold_by_sf:
            for g in gold_by_sf[sf_norm]:
                gold_key = (g.sf_normalized, g.lf_normalized)
                if gold_key not in matched_gold:
                    if lf_matches(ext.lf_normalized, g.lf_normalized, FUZZY_THRESHOLD):
                        metrics.tp += 1
                        metrics.tp_pairs.append((ext.short_form, ext.long_form or ""))
                        matched_gold.add(gold_key)
                        matched = True
                        break

        if not matched:
            metrics.fp += 1
            metrics.fp_pairs.append((ext.short_form, ext.long_form or ""))

    for g in gold:
        gold_key = (g.sf_normalized, g.lf_normalized)
        if gold_key not in matched_gold:
            metrics.fn += 1
            metrics.fn_pairs.append((g.short_form, g.long_form))

    return metrics, class_counts


# =============================================================================
# ORCHESTRATOR RUNNER
# =============================================================================


def create_orchestrator() -> Orchestrator:
    """Create and initialize orchestrator once (expensive operation)."""
    return Orchestrator(
        config_path=str(CONFIG_PATH),
        gold_json=str(GOLD_JSON),
        skip_validation=SKIP_VALIDATION,
    )


def run_orchestrator(orch: Orchestrator, pdf_path: Path) -> List[ExtractedEntry]:
    """Run the extraction pipeline on a single PDF using existing orchestrator."""
    results = orch.process_pdf(str(pdf_path))

    extracted = []
    for entity in results:
        if entity.status == ValidationStatus.VALIDATED:
            # Get lexicon source if available
            lexicon_source = None
            if hasattr(entity, 'provenance') and entity.provenance:
                lexicon_source = getattr(entity.provenance, 'lexicon_source', None)

            # Classify the abbreviation
            abbrev_class = classify_abbreviation(
                short_form=entity.short_form,
                long_form=entity.long_form,
                lexicon_source=lexicon_source,
            )

            extracted.append(
                ExtractedEntry(
                    short_form=entity.short_form,
                    long_form=entity.long_form,
                    confidence=entity.confidence_score,
                    abbrev_class=abbrev_class,
                    lexicon_source=lexicon_source,
                )
            )

    return extracted


# =============================================================================
# EVALUATION RUNNER
# =============================================================================


def evaluate_split(
    split_name: str,
    split_path: Path,
    gold_by_doc: Dict[str, List[GoldEntry]],
    orch: Orchestrator,
) -> Tuple[AggregateMetrics, List[DocumentMetrics]]:
    """Evaluate all PDFs in a split using shared orchestrator instance."""
    print(f"\n{'='*70}")
    print(f" EVALUATING SPLIT: {split_name.upper()}")
    print(f"{'='*70}")

    if not split_path.exists():
        print(f"  [WARN] Split path not found: {split_path}")
        return AggregateMetrics(split_name=split_name), []

    pdf_files = sorted(split_path.glob("*.pdf"))
    pdf_with_gold = [pdf for pdf in pdf_files if pdf.name in gold_by_doc]
    total_with_gold = len(pdf_with_gold)  # Count before truncation

    if MAX_DOCS:
        pdf_with_gold = pdf_with_gold[:MAX_DOCS]

    print(f"  PDFs in folder:    {len(pdf_files)}")
    print(f"  PDFs with gold:    {total_with_gold}")
    print(f"  PDFs to process:   {len(pdf_with_gold)}")

    aggregate = AggregateMetrics(split_name=split_name)
    aggregate.total_docs = len(pdf_with_gold)
    doc_metrics_list = []

    for i, pdf_path in enumerate(pdf_with_gold, 1):
        print(f"\n  [{i}/{len(pdf_with_gold)}] {pdf_path.name}")

        gold_entries = gold_by_doc.get(pdf_path.name, [])
        print(f"      Gold annotations: {len(gold_entries)}")

        start_time = time.time()
        
        try:
            extracted = run_orchestrator(orch, pdf_path)
            elapsed = time.time() - start_time

            print(f"      Extracted:        {len(extracted)}")
            print(f"      Time:             {elapsed:.1f}s")

            metrics, class_counts = compare_extractions(extracted, gold_entries, eval_classes=EVAL_ABBREV_CLASSES)
            metrics.processing_time = elapsed
            doc_metrics_list.append(metrics)

            aggregate.processed_docs += 1
            aggregate.total_tp += metrics.tp
            aggregate.total_fp += metrics.fp
            aggregate.total_fn += metrics.fn
            aggregate.total_gold += metrics.gold_count
            aggregate.total_extracted += metrics.extracted_count
            aggregate.total_time += elapsed

            print(f"      Results:          TP={metrics.tp}, FP={metrics.fp}, FN={metrics.fn}")
            print(f"      Metrics:          P={metrics.precision:.1%}, R={metrics.recall:.1%}, F1={metrics.f1:.1%}")

        except Exception as e:
            print(f"      [ERROR] {e}")
            metrics = DocumentMetrics(doc_id=pdf_path.name, error=str(e))
            doc_metrics_list.append(metrics)
            aggregate.failed_docs += 1

    return aggregate, doc_metrics_list


# =============================================================================
# REPORTING
# =============================================================================


def print_split_report(metrics: AggregateMetrics):
    """Print summary report for a split."""
    print(f"\n{'─'*70}")
    print(f" SPLIT SUMMARY: {metrics.split_name.upper()}")
    print(f"{'─'*70}")
    print(f"  Documents processed: {metrics.processed_docs}/{metrics.total_docs}")
    if metrics.failed_docs > 0:
        print(f"  Documents failed:    {metrics.failed_docs}")
    print(f"  Total time:          {metrics.total_time:.1f}s")
    print()
    print(f"  Gold entries:        {metrics.total_gold}")
    print(f"  Extracted:           {metrics.total_extracted}")
    print()
    print(f"  ┌─────────────────────────────────────┐")
    print(f"  │  True Positives (TP):  {metrics.total_tp:>5}        │")
    print(f"  │  False Positives (FP): {metrics.total_fp:>5}        │")
    print(f"  │  False Negatives (FN): {metrics.total_fn:>5}        │")
    print(f"  ├─────────────────────────────────────┤")
    print(f"  │  Precision:           {metrics.precision:>6.1%}       │")
    print(f"  │  Recall:              {metrics.recall:>6.1%}       │")
    print(f"  │  F1 Score:            {metrics.f1:>6.1%}       │")
    print(f"  └─────────────────────────────────────┘")


def print_error_analysis(doc_list: List[DocumentMetrics], max_examples: int = 15):
    """Print detailed error analysis."""
    all_fn = [(doc.doc_id, sf, lf) for doc in doc_list for sf, lf in doc.fn_pairs]
    all_fp = [(doc.doc_id, sf, lf) for doc in doc_list for sf, lf in doc.fp_pairs]

    print(f"\n{'='*70}")
    print(f" ERROR ANALYSIS")
    print(f"{'='*70}")

    print(f"\n FALSE NEGATIVES (Missed) - Total: {len(all_fn)}")
    print(f" {'─'*68}")
    if all_fn:
        print(f"  {'SF':<12} {'Long Form':<35} {'Document':<20}")
        print(f"  {'─'*12} {'─'*35} {'─'*20}")
        for doc_id, sf, lf in all_fn[:max_examples]:
            lf_short = (lf[:32] + "...") if len(lf) > 35 else lf
            doc_short = (doc_id[:17] + "...") if len(doc_id) > 20 else doc_id
            print(f"  {sf:<12} {lf_short:<35} {doc_short:<20}")
        if len(all_fn) > max_examples:
            print(f"  ... and {len(all_fn) - max_examples} more")
    else:
        print("  (none)")

    print(f"\n FALSE POSITIVES (Extra) - Total: {len(all_fp)}")
    print(f" {'─'*68}")
    if all_fp:
        print(f"  {'SF':<12} {'Long Form':<35} {'Document':<20}")
        print(f"  {'─'*12} {'─'*35} {'─'*20}")
        for doc_id, sf, lf in all_fp[:max_examples]:
            lf_short = ((lf[:32] + "...") if len(lf) > 35 else lf) if lf else "(no LF)"
            doc_short = (doc_id[:17] + "...") if len(doc_id) > 20 else doc_id
            print(f"  {sf:<12} {lf_short:<35} {doc_short:<20}")
        if len(all_fp) > max_examples:
            print(f"  ... and {len(all_fp) - max_examples} more")
    else:
        print("  (none)")


# =============================================================================
# MAIN
# =============================================================================


def main():
    print("="*70)
    print(" NLP4RARE PIPELINE EVALUATION")
    print("="*70)
    print(f" Gold standard:    {GOLD_JSON}")
    print(f" NLP4RARE path:    {NLP4RARE_PATH}")
    print(f" Config:           {CONFIG_PATH}")
    print(f" Skip validation:  {SKIP_VALIDATION}")
    print(f" Max docs/split:   {MAX_DOCS}")
    print(f" Splits:           {SPLITS_TO_EVALUATE}")
    print(f" Gold categories:  {GOLD_CATEGORIES}")
    print(f" Eval classes:     {EVAL_ABBREV_CLASSES}")
    print("="*70)

    # Load gold standard (filtered by category)
    print("\n Loading gold standard...")
    gold_by_doc = load_gold_standard(GOLD_JSON, categories=GOLD_CATEGORIES)
    print(f" Loaded {sum(len(v) for v in gold_by_doc.values())} annotations across {len(gold_by_doc)} documents")

    # Initialize orchestrator ONCE (expensive: loads scispacy, UMLS, lexicons)
    print("\n Initializing orchestrator (one-time cost)...")
    init_start = time.time()
    orch = create_orchestrator()
    init_time = time.time() - init_start
    print(f" Orchestrator initialized in {init_time:.1f}s")

    # Run evaluation
    all_results = []
    for split_name in SPLITS_TO_EVALUATE:
        split_path = NLP4RARE_PATH / split_name
        aggregate, doc_list = evaluate_split(split_name, split_path, gold_by_doc, orch)
        all_results.append((aggregate, doc_list))
        print_split_report(aggregate)

    # Overall metrics (if multiple splits)
    if len(all_results) > 1:
        print(f"\n{'='*70}")
        print(" OVERALL METRICS (ALL SPLITS)")
        print(f"{'='*70}")
        
        total_tp = sum(a.total_tp for a, _ in all_results)
        total_fp = sum(a.total_fp for a, _ in all_results)
        total_fn = sum(a.total_fn for a, _ in all_results)
        total_docs = sum(a.processed_docs for a, _ in all_results)
        total_time = sum(a.total_time for a, _ in all_results)

        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        print(f"  Documents: {total_docs}")
        print(f"  TP: {total_tp}  |  FP: {total_fp}  |  FN: {total_fn}")
        print(f"  Precision: {precision:.1%}  |  Recall: {recall:.1%}  |  F1: {f1:.1%}")
        print(f"  Total time: {total_time:.1f}s")

    # Error analysis
    all_docs = [doc for _, doc_list in all_results for doc in doc_list]
    print_error_analysis(all_docs)

    # Timing summary
    total_processing = sum(a.total_time for a, _ in all_results)
    total_docs = sum(a.processed_docs for a, _ in all_results)
    print(f"\n{'='*70}")
    print(" TIMING SUMMARY")
    print(f"{'='*70}")
    print(f"  Orchestrator init:   {init_time:.1f}s (one-time)")
    print(f"  Document processing: {total_processing:.1f}s ({total_docs} docs)")
    if total_docs > 0:
        print(f"  Avg per document:    {total_processing / total_docs:.1f}s")
    print(f"  Total elapsed:       {init_time + total_processing:.1f}s")

    print(f"\n{'='*70}")
    print(" EVALUATION COMPLETE")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()






