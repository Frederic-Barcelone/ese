#!/usr/bin/env python3
# corpus_metadata/corpus_metadata/F_evaluation/F03_generator_unit_test.py
"""
================================================================================
NLP4RARE PIPELINE EVALUATION TEST
================================================================================

PURPOSE:
    End-to-end evaluation of the abbreviation extraction pipeline against
    the NLP4RARE gold standard corpus. This script validates that the full
    pipeline (parsing → generation → validation → normalization) correctly
    extracts abbreviation-definition pairs from medical documents.

WHAT IT DOES:
    1. LOAD GOLD STANDARD
       - Reads nlp4rare_gold.json containing human-annotated abbreviation pairs
       - Groups annotations by document (e.g., "Acanthosis-Nigricans.pdf")
       - Each gold entry has: short_form, long_form, category, doc_id

    2. PROCESS PDFs
       - For each PDF file in the NLP4RARE corpus (dev/test/train folders)
       - Runs the full Orchestrator pipeline:
         * B_parsing: PDF → DocumentGraph (layout-aware text extraction)
         * C_generators: Candidate generation (syntax, regex, lexicon, glossary)
         * D_validation: LLM verification (optional, can be skipped for speed)
         * E_normalization: Term mapping and disambiguation
       - Collects validated abbreviation extractions

    3. COMPARE RESULTS
       - For each document, compares extracted abbreviations vs gold standard
       - Matching logic:
         * Short Form: Exact match (case-insensitive)
         * Long Form: Fuzzy match (exact, substring, or 80% similarity)
       - Classifies each result as TP (correct), FP (extra), or FN (missed)

    4. CALCULATE METRICS
       - Per-document: TP, FP, FN, Precision, Recall, F1
       - Per-split (dev/test/train): Aggregate metrics
       - Overall corpus: Combined metrics across all splits

METRICS EXPLAINED:
    TP (True Positive):  System extracted abbreviation that matches gold standard
    FP (False Positive): System extracted abbreviation NOT in gold (noise/hallucination)
    FN (False Negative): Gold standard abbreviation NOT extracted (missed)
    
    Precision = TP / (TP + FP)  → "Of what we found, how much is correct?"
    Recall    = TP / (TP + FN)  → "Of what exists, how much did we find?"
    F1 Score  = 2 * P * R / (P + R)  → Harmonic mean (balance of P and R)

OUTPUT:
    Console only - prints detailed progress and final metrics report including:
    - Per-document extraction counts and metrics
    - Per-split summary tables
    - Overall corpus performance
    - Error analysis with sample FN (missed) and FP (extra) pairs

CONFIGURATION:
    All parameters are hardcoded in the CONFIGURATION section below.
    Modify MAX_DOCS to control how many documents to process (for testing).
    Set SKIP_VALIDATION=True for faster runs (skips LLM verification).

USAGE:
    python F04_pipeline_test.py

DEPENDENCIES:
    - Orchestrator and all pipeline components (A_core through E_normalization)
    - nlp4rare_gold.json (generated by generate_nlp4rare_gold.py)
    - NLP4RARE PDF files in dev/test/train folders

================================================================================
"""

import json
import sys
import time
from dataclasses import dataclass, field
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# =============================================================================
# CONFIGURATION (ALL HARDCODED - MODIFY HERE)
# =============================================================================

BASE_PATH = Path("/Users/frederictetard/Projects/ese")
GOLD_JSON = BASE_PATH / "gold_data" / "nlp4rare_gold.json"
NLP4RARE_PATH = BASE_PATH / "gold_data" / "NLP4RARE"
CONFIG_PATH = BASE_PATH / "corpus_metadata" / "G_config" / "config.yaml"

# Evaluation settings
MAX_DOCS = 5                    # Max documents to process per split (None = all)
SKIP_VALIDATION = False         # Enable LLM validation for proper evaluation
SPLITS_TO_EVALUATE = ["dev"]    # Which splits to run: ["dev"], ["test"], ["train"], or all
FUZZY_THRESHOLD = 0.8           # Long form matching threshold (0.8 = 80% similarity)


# =============================================================================
# DATA STRUCTURES
# =============================================================================


@dataclass
class GoldEntry:
    """Single gold standard annotation."""
    short_form: str
    long_form: str
    category: str
    doc_id: str

    @property
    def sf_normalized(self) -> str:
        return self.short_form.strip().upper()

    @property
    def lf_normalized(self) -> str:
        return " ".join(self.long_form.strip().lower().split())


@dataclass
class ExtractedEntry:
    """Single extracted abbreviation."""
    short_form: str
    long_form: Optional[str]
    confidence: float = 0.0

    @property
    def sf_normalized(self) -> str:
        return self.short_form.strip().upper()

    @property
    def lf_normalized(self) -> Optional[str]:
        if not self.long_form:
            return None
        return " ".join(self.long_form.strip().lower().split())


@dataclass
class DocumentMetrics:
    """Metrics for a single document."""
    doc_id: str
    tp: int = 0
    fp: int = 0
    fn: int = 0
    gold_count: int = 0
    extracted_count: int = 0
    processing_time: float = 0.0
    error: Optional[str] = None
    tp_pairs: List[Tuple[str, str]] = field(default_factory=list)
    fp_pairs: List[Tuple[str, str]] = field(default_factory=list)
    fn_pairs: List[Tuple[str, str]] = field(default_factory=list)

    @property
    def precision(self) -> float:
        return self.tp / (self.tp + self.fp) if (self.tp + self.fp) > 0 else 0.0

    @property
    def recall(self) -> float:
        return self.tp / (self.tp + self.fn) if (self.tp + self.fn) > 0 else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


@dataclass
class AggregateMetrics:
    """Aggregate metrics across multiple documents."""
    split_name: str
    total_docs: int = 0
    processed_docs: int = 0
    failed_docs: int = 0
    total_tp: int = 0
    total_fp: int = 0
    total_fn: int = 0
    total_gold: int = 0
    total_extracted: int = 0
    total_time: float = 0.0

    @property
    def precision(self) -> float:
        return self.total_tp / (self.total_tp + self.total_fp) if (self.total_tp + self.total_fp) > 0 else 0.0

    @property
    def recall(self) -> float:
        return self.total_tp / (self.total_tp + self.total_fn) if (self.total_tp + self.total_fn) > 0 else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


# =============================================================================
# GOLD STANDARD LOADER
# =============================================================================


def load_gold_standard(gold_path: Path) -> Dict[str, List[GoldEntry]]:
    """Load gold standard and group by document ID."""
    if not gold_path.exists():
        raise FileNotFoundError(f"Gold standard not found: {gold_path}")

    with open(gold_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    annotations = data.get("annotations", [])
    
    by_doc: Dict[str, List[GoldEntry]] = {}
    for ann in annotations:
        entry = GoldEntry(
            short_form=ann["short_form"],
            long_form=ann["long_form"],
            category=ann.get("category", "UNKNOWN"),
            doc_id=ann["doc_id"],
        )
        by_doc.setdefault(entry.doc_id, []).append(entry)

    return by_doc


# =============================================================================
# MATCHING LOGIC
# =============================================================================


def lf_matches(sys_lf: Optional[str], gold_lf: Optional[str], threshold: float = 0.8) -> bool:
    """Check if long forms match (exact, substring, or fuzzy)."""
    if sys_lf is None and gold_lf is None:
        return True
    if sys_lf is None or gold_lf is None:
        return False
    
    sys_norm = " ".join(sys_lf.strip().lower().split())
    gold_norm = " ".join(gold_lf.strip().lower().split())
    
    if sys_norm == gold_norm:
        return True
    if sys_norm in gold_norm or gold_norm in sys_norm:
        return True
    
    ratio = SequenceMatcher(None, sys_norm, gold_norm).ratio()
    return ratio >= threshold


def compare_extractions(
    extracted: List[ExtractedEntry],
    gold: List[GoldEntry],
) -> DocumentMetrics:
    """Compare extracted abbreviations against gold standard."""
    doc_id = gold[0].doc_id if gold else "unknown"
    metrics = DocumentMetrics(doc_id=doc_id)
    metrics.gold_count = len(gold)
    metrics.extracted_count = len(extracted)

    gold_by_sf: Dict[str, List[GoldEntry]] = {}
    for g in gold:
        gold_by_sf.setdefault(g.sf_normalized, []).append(g)

    matched_gold: Set[Tuple[str, str]] = set()

    for ext in extracted:
        sf_norm = ext.sf_normalized
        matched = False
        
        if sf_norm in gold_by_sf:
            for g in gold_by_sf[sf_norm]:
                gold_key = (g.sf_normalized, g.lf_normalized)
                if gold_key not in matched_gold:
                    if lf_matches(ext.lf_normalized, g.lf_normalized, FUZZY_THRESHOLD):
                        metrics.tp += 1
                        metrics.tp_pairs.append((ext.short_form, ext.long_form or ""))
                        matched_gold.add(gold_key)
                        matched = True
                        break

        if not matched:
            metrics.fp += 1
            metrics.fp_pairs.append((ext.short_form, ext.long_form or ""))

    for g in gold:
        gold_key = (g.sf_normalized, g.lf_normalized)
        if gold_key not in matched_gold:
            metrics.fn += 1
            metrics.fn_pairs.append((g.short_form, g.long_form))

    return metrics


# =============================================================================
# ORCHESTRATOR RUNNER
# =============================================================================


def run_orchestrator(pdf_path: Path) -> List[ExtractedEntry]:
    """Run the extraction pipeline on a single PDF."""
    sys.path.insert(0, str(BASE_PATH / "corpus_metadata"))
    
    from orchestrator import Orchestrator
    from A_core.A01_domain_models import ValidationStatus

    orch = Orchestrator(
        config_path=str(CONFIG_PATH),
        gold_json=str(GOLD_JSON),
        skip_validation=SKIP_VALIDATION,
    )

    results = orch.process_pdf(str(pdf_path))

    extracted = []
    for entity in results:
        if entity.status == ValidationStatus.VALIDATED:
            extracted.append(
                ExtractedEntry(
                    short_form=entity.short_form,
                    long_form=entity.long_form,
                    confidence=entity.confidence_score,
                )
            )

    return extracted


# =============================================================================
# EVALUATION RUNNER
# =============================================================================


def evaluate_split(
    split_name: str,
    split_path: Path,
    gold_by_doc: Dict[str, List[GoldEntry]],
) -> Tuple[AggregateMetrics, List[DocumentMetrics]]:
    """Evaluate all PDFs in a split."""
    print(f"\n{'='*70}")
    print(f" EVALUATING SPLIT: {split_name.upper()}")
    print(f"{'='*70}")

    if not split_path.exists():
        print(f"  [WARN] Split path not found: {split_path}")
        return AggregateMetrics(split_name=split_name), []

    pdf_files = sorted(split_path.glob("*.pdf"))
    pdf_with_gold = [pdf for pdf in pdf_files if pdf.name in gold_by_doc]

    if MAX_DOCS:
        pdf_with_gold = pdf_with_gold[:MAX_DOCS]

    print(f"  PDFs in folder:    {len(pdf_files)}")
    print(f"  PDFs with gold:    {len([p for p in pdf_files if p.name in gold_by_doc])}")
    print(f"  PDFs to process:   {len(pdf_with_gold)}")

    aggregate = AggregateMetrics(split_name=split_name)
    aggregate.total_docs = len(pdf_with_gold)
    doc_metrics_list = []

    for i, pdf_path in enumerate(pdf_with_gold, 1):
        print(f"\n  [{i}/{len(pdf_with_gold)}] {pdf_path.name}")

        gold_entries = gold_by_doc.get(pdf_path.name, [])
        print(f"      Gold annotations: {len(gold_entries)}")

        start_time = time.time()
        
        try:
            extracted = run_orchestrator(pdf_path)
            elapsed = time.time() - start_time

            print(f"      Extracted:        {len(extracted)}")
            print(f"      Time:             {elapsed:.1f}s")

            metrics = compare_extractions(extracted, gold_entries)
            metrics.processing_time = elapsed
            doc_metrics_list.append(metrics)

            aggregate.processed_docs += 1
            aggregate.total_tp += metrics.tp
            aggregate.total_fp += metrics.fp
            aggregate.total_fn += metrics.fn
            aggregate.total_gold += metrics.gold_count
            aggregate.total_extracted += metrics.extracted_count
            aggregate.total_time += elapsed

            print(f"      Results:          TP={metrics.tp}, FP={metrics.fp}, FN={metrics.fn}")
            print(f"      Metrics:          P={metrics.precision:.1%}, R={metrics.recall:.1%}, F1={metrics.f1:.1%}")

        except Exception as e:
            print(f"      [ERROR] {e}")
            metrics = DocumentMetrics(doc_id=pdf_path.name, error=str(e))
            doc_metrics_list.append(metrics)
            aggregate.failed_docs += 1

    return aggregate, doc_metrics_list


# =============================================================================
# REPORTING
# =============================================================================


def print_split_report(metrics: AggregateMetrics):
    """Print summary report for a split."""
    print(f"\n{'─'*70}")
    print(f" SPLIT SUMMARY: {metrics.split_name.upper()}")
    print(f"{'─'*70}")
    print(f"  Documents processed: {metrics.processed_docs}/{metrics.total_docs}")
    if metrics.failed_docs > 0:
        print(f"  Documents failed:    {metrics.failed_docs}")
    print(f"  Total time:          {metrics.total_time:.1f}s")
    print()
    print(f"  Gold entries:        {metrics.total_gold}")
    print(f"  Extracted:           {metrics.total_extracted}")
    print()
    print(f"  ┌─────────────────────────────────────┐")
    print(f"  │  True Positives (TP):  {metrics.total_tp:>5}        │")
    print(f"  │  False Positives (FP): {metrics.total_fp:>5}        │")
    print(f"  │  False Negatives (FN): {metrics.total_fn:>5}        │")
    print(f"  ├─────────────────────────────────────┤")
    print(f"  │  Precision:           {metrics.precision:>6.1%}       │")
    print(f"  │  Recall:              {metrics.recall:>6.1%}       │")
    print(f"  │  F1 Score:            {metrics.f1:>6.1%}       │")
    print(f"  └─────────────────────────────────────┘")


def print_error_analysis(doc_list: List[DocumentMetrics], max_examples: int = 15):
    """Print detailed error analysis."""
    all_fn = [(doc.doc_id, sf, lf) for doc in doc_list for sf, lf in doc.fn_pairs]
    all_fp = [(doc.doc_id, sf, lf) for doc in doc_list for sf, lf in doc.fp_pairs]

    print(f"\n{'='*70}")
    print(f" ERROR ANALYSIS")
    print(f"{'='*70}")

    print(f"\n FALSE NEGATIVES (Missed) - Total: {len(all_fn)}")
    print(f" {'─'*68}")
    if all_fn:
        print(f"  {'SF':<12} {'Long Form':<35} {'Document':<20}")
        print(f"  {'─'*12} {'─'*35} {'─'*20}")
        for doc_id, sf, lf in all_fn[:max_examples]:
            lf_short = (lf[:32] + "...") if len(lf) > 35 else lf
            doc_short = (doc_id[:17] + "...") if len(doc_id) > 20 else doc_id
            print(f"  {sf:<12} {lf_short:<35} {doc_short:<20}")
        if len(all_fn) > max_examples:
            print(f"  ... and {len(all_fn) - max_examples} more")
    else:
        print("  (none)")

    print(f"\n FALSE POSITIVES (Extra) - Total: {len(all_fp)}")
    print(f" {'─'*68}")
    if all_fp:
        print(f"  {'SF':<12} {'Long Form':<35} {'Document':<20}")
        print(f"  {'─'*12} {'─'*35} {'─'*20}")
        for doc_id, sf, lf in all_fp[:max_examples]:
            lf_short = ((lf[:32] + "...") if len(lf) > 35 else lf) if lf else "(no LF)"
            doc_short = (doc_id[:17] + "...") if len(doc_id) > 20 else doc_id
            print(f"  {sf:<12} {lf_short:<35} {doc_short:<20}")
        if len(all_fp) > max_examples:
            print(f"  ... and {len(all_fp) - max_examples} more")
    else:
        print("  (none)")


# =============================================================================
# MAIN
# =============================================================================


def main():
    print("="*70)
    print(" NLP4RARE PIPELINE EVALUATION")
    print("="*70)
    print(f" Gold standard:    {GOLD_JSON}")
    print(f" NLP4RARE path:    {NLP4RARE_PATH}")
    print(f" Config:           {CONFIG_PATH}")
    print(f" Skip validation:  {SKIP_VALIDATION}")
    print(f" Max docs/split:   {MAX_DOCS}")
    print(f" Splits:           {SPLITS_TO_EVALUATE}")
    print("="*70)

    # Load gold standard
    print("\n Loading gold standard...")
    gold_by_doc = load_gold_standard(GOLD_JSON)
    print(f" Loaded {sum(len(v) for v in gold_by_doc.values())} annotations across {len(gold_by_doc)} documents")

    # Run evaluation
    all_results = []
    for split_name in SPLITS_TO_EVALUATE:
        split_path = NLP4RARE_PATH / split_name
        aggregate, doc_list = evaluate_split(split_name, split_path, gold_by_doc)
        all_results.append((aggregate, doc_list))
        print_split_report(aggregate)

    # Overall metrics (if multiple splits)
    if len(all_results) > 1:
        print(f"\n{'='*70}")
        print(" OVERALL METRICS (ALL SPLITS)")
        print(f"{'='*70}")
        
        total_tp = sum(a.total_tp for a, _ in all_results)
        total_fp = sum(a.total_fp for a, _ in all_results)
        total_fn = sum(a.total_fn for a, _ in all_results)
        total_docs = sum(a.processed_docs for a, _ in all_results)
        total_time = sum(a.total_time for a, _ in all_results)

        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        print(f"  Documents: {total_docs}")
        print(f"  TP: {total_tp}  |  FP: {total_fp}  |  FN: {total_fn}")
        print(f"  Precision: {precision:.1%}  |  Recall: {recall:.1%}  |  F1: {f1:.1%}")
        print(f"  Total time: {total_time:.1f}s")

    # Error analysis
    all_docs = [doc for _, doc_list in all_results for doc in doc_list]
    print_error_analysis(all_docs)

    print(f"\n{'='*70}")
    print(" EVALUATION COMPLETE")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()






